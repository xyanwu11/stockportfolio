import streamlit as st
import pandas as pd
import numpy as np
from functools import wraps
import traceback
import logging
from datetime import datetime

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('portfolio_analysis.log'),
        logging.StreamHandler()
    ]
)

def error_handler(func):
    """é€šç”¨éŒ¯èª¤è™•ç†è£é£¾å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            error_msg = f"å‡½æ•¸ {func.__name__} ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            logging.error(f"{error_msg}\n{traceback.format_exc()}")
            
            # åœ¨Streamlitä¸­é¡¯ç¤ºå‹å–„çš„éŒ¯èª¤è¨Šæ¯
            st.error(f"âŒ {error_msg}")
            st.expander("è©³ç´°éŒ¯èª¤è³‡è¨Š", expanded=False).code(traceback.format_exc())
            
            return None
    return wrapper

def validate_data(df, required_columns, df_name="DataFrame"):
    """é©—è­‰DataFrameçš„å®Œæ•´æ€§"""
    if df is None or df.empty:
        st.error(f"âŒ {df_name} ç‚ºç©ºæˆ–ä¸å­˜åœ¨")
        return False
    
    missing_columns = []
    for i, col in enumerate(required_columns):
        if i >= len(df.columns):
            missing_columns.append(f"ç¬¬{i+1}åˆ—({col})")
    
    if missing_columns:
        st.warning(f"âš ï¸ {df_name} ç¼ºå°‘å¿…è¦æ¬„ä½: {', '.join(missing_columns)}")
        return False
    
    return True

def safe_calculate_metrics(returns, metric_name="ç¸¾æ•ˆæŒ‡æ¨™"):
    """å®‰å…¨è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™"""
    try:
        if returns is None or len(returns) == 0:
            st.warning(f"âš ï¸ {metric_name} è¨ˆç®—å¤±æ•—: å ±é…¬ç‡æ•¸æ“šç‚ºç©º")
            return {}
        
        if returns.isna().all():
            st.warning(f"âš ï¸ {metric_name} è¨ˆç®—å¤±æ•—: æ‰€æœ‰å ±é…¬ç‡æ•¸æ“šéƒ½æ˜¯NaN")
            return {}
        
        # ç§»é™¤NaNå€¼
        clean_returns = returns.dropna()
        if len(clean_returns) == 0:
            st.warning(f"âš ï¸ {metric_name} è¨ˆç®—å¤±æ•—: æ¸…ç†å¾Œçš„å ±é…¬ç‡æ•¸æ“šç‚ºç©º")
            return {}
        
        metrics = {}
        
        # åŸºæœ¬çµ±è¨ˆ
        try:
            metrics['ç¸½å ±é…¬ç‡'] = (1 + clean_returns).prod() - 1
        except:
            metrics['ç¸½å ±é…¬ç‡'] = 0
        
        try:
            metrics['å¹´åŒ–å ±é…¬ç‡'] = (1 + clean_returns.mean()) ** 252 - 1
        except:
            metrics['å¹´åŒ–å ±é…¬ç‡'] = 0
        
        try:
            metrics['å¹´åŒ–æ³¢å‹•ç‡'] = clean_returns.std() * np.sqrt(252)
        except:
            metrics['å¹´åŒ–æ³¢å‹•ç‡'] = 0
        
        try:
            metrics['å¤æ™®æ¯”ç‡'] = metrics['å¹´åŒ–å ±é…¬ç‡'] / metrics['å¹´åŒ–æ³¢å‹•ç‡'] if metrics['å¹´åŒ–æ³¢å‹•ç‡'] != 0 else 0
        except:
            metrics['å¤æ™®æ¯”ç‡'] = 0
        
        # æœ€å¤§å›æ’¤
        try:
            cumulative = (1 + clean_returns).cumprod()
            rolling_max = cumulative.expanding().max()
            drawdown = (cumulative - rolling_max) / rolling_max
            metrics['æœ€å¤§å›æ’¤'] = drawdown.min()
        except:
            metrics['æœ€å¤§å›æ’¤'] = 0
        
        # å‹ç‡
        try:
            metrics['å‹ç‡'] = (clean_returns > 0).mean()
        except:
            metrics['å‹ç‡'] = 0.5
        
        # VaR
        try:
            metrics['VaR_95%'] = clean_returns.quantile(0.05)
        except:
            metrics['VaR_95%'] = 0
        
        # ç´¢æè«¾æ¯”ç‡
        try:
            downside_returns = clean_returns[clean_returns < 0]
            if len(downside_returns) > 0:
                downside_deviation = downside_returns.std() * np.sqrt(252)
                metrics['ç´¢æè«¾æ¯”ç‡'] = metrics['å¹´åŒ–å ±é…¬ç‡'] / downside_deviation if downside_deviation != 0 else 0
            else:
                metrics['ç´¢æè«¾æ¯”ç‡'] = metrics['å¹´åŒ–å ±é…¬ç‡'] * 10  # å¦‚æœæ²’æœ‰è² å ±é…¬ï¼Œçµ¦ä¸€å€‹é«˜åˆ†
        except:
            metrics['ç´¢æè«¾æ¯”ç‡'] = 0
        
        return metrics
        
    except Exception as e:
        st.error(f"âŒ {metric_name} è¨ˆç®—æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {str(e)}")
        logging.error(f"è¨ˆç®— {metric_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\n{traceback.format_exc()}")
        return {}

def show_data_quality_info(df, name):
    """é¡¯ç¤ºæ•¸æ“šå“è³ªè³‡è¨Š"""
    if df is None or df.empty:
        st.warning(f"âš ï¸ {name} æ•¸æ“šç‚ºç©º")
        return
    
    with st.expander(f"ğŸ“Š {name} æ•¸æ“šå“è³ªè³‡è¨Š", expanded=False):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("ç¸½è¡Œæ•¸", len(df))
            st.metric("ç¸½åˆ—æ•¸", len(df.columns))
        
        with col2:
            nan_count = df.isna().sum().sum()
            st.metric("NaNå€¼ç¸½æ•¸", nan_count)
            coverage = (1 - nan_count / (len(df) * len(df.columns))) * 100
            st.metric("æ•¸æ“šè¦†è“‹ç‡", f"{coverage:.1f}%")
        
        with col3:
            if len(df.columns) > 0:
                date_range = f"{df.index[0].strftime('%Y-%m-%d')} è‡³ {df.index[-1].strftime('%Y-%m-%d')}" if hasattr(df.index[0], 'strftime') else "æœªçŸ¥æ—¥æœŸç¯„åœ"
                st.write(f"**æ—¥æœŸç¯„åœ:**\n{date_range}")

def format_percentage(value, decimals=2):
    """å®‰å…¨æ ¼å¼åŒ–ç™¾åˆ†æ¯”"""
    try:
        if pd.isna(value) or value == float('inf') or value == float('-inf'):
            return "N/A"
        return f"{value:.{decimals}%}"
    except:
        return "N/A"

def format_number(value, decimals=2):
    """å®‰å…¨æ ¼å¼åŒ–æ•¸å­—"""
    try:
        if pd.isna(value) or value == float('inf') or value == float('-inf'):
            return "N/A"
        return f"{value:.{decimals}f}"
    except:
        return "N/A"

def create_fallback_chart_data():
    """å‰µå»ºå‚™ç”¨åœ–è¡¨æ•¸æ“š"""
    dates = pd.date_range(start='2020-01-01', end='2024-12-31', freq='D')
    data = {
        'é«˜å ±é…¬ç­–ç•¥': np.random.normal(1.0002, 0.02, len(dates)).cumprod(),
        'ä½é¢¨éšªç­–ç•¥': np.random.normal(1.0001, 0.015, len(dates)).cumprod(),
    }
    return pd.DataFrame(data, index=dates)

@error_handler 
def safe_portfolio_calculation(stock_data, weights, portfolio_name):
    """å®‰å…¨è¨ˆç®—æŠ•è³‡çµ„åˆå ±é…¬"""
    if stock_data is None or stock_data.empty:
        st.error(f"âŒ {portfolio_name} è‚¡ç¥¨æ•¸æ“šç‚ºç©º")
        return pd.Series()
    
    if not weights:
        st.error(f"âŒ {portfolio_name} æ¬Šé‡è³‡æ–™ç‚ºç©º")
        return pd.Series()
    
    # æª¢æŸ¥æ¬Šé‡ç¸½å’Œ
    weight_sum = sum(weights.values())
    if abs(weight_sum - 1.0) > 0.01:
        st.warning(f"âš ï¸ {portfolio_name} æ¬Šé‡ç¸½å’Œç‚º {weight_sum:.3f}ï¼Œè‡ªå‹•æ¨™æº–åŒ–è‡³1.0")
        weights = {k: v/weight_sum for k, v in weights.items()}
    
    # è¨ˆç®—å ±é…¬ç‡
    returns = stock_data.pct_change().dropna()
    
    if returns.empty:
        st.error(f"âŒ {portfolio_name} å ±é…¬ç‡è¨ˆç®—å¤±æ•—")
        return pd.Series()
    
    # ç¢ºä¿æ‰€æœ‰æ¬Šé‡å°æ‡‰çš„è‚¡ç¥¨éƒ½å­˜åœ¨
    missing_stocks = set(weights.keys()) - set(returns.columns)
    if missing_stocks:
        st.warning(f"âš ï¸ {portfolio_name} ç¼ºå°‘ä»¥ä¸‹è‚¡ç¥¨æ•¸æ“š: {missing_stocks}")
        weights = {k: v for k, v in weights.items() if k not in missing_stocks}
    
    if not weights:
        st.error(f"âŒ {portfolio_name} æ²’æœ‰å¯ç”¨çš„è‚¡ç¥¨æ•¸æ“š")
        return pd.Series()
    
    # è¨ˆç®—æŠ•è³‡çµ„åˆå ±é…¬ç‡
    portfolio_returns = (returns[list(weights.keys())] * pd.Series(weights)).sum(axis=1)
    
    return portfolio_returns